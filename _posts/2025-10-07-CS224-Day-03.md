---
layout:     post
title:      CS224Day-03
subtitle:   LSTM
date:       2025-10-07
author:     Jeffzzc
header-img: img/home-bg-sky2.jpg
catalog: true
tags:
    - CS224
---
### Lecture 6 LSTM

#### LSTM

LSTM这个很难记的网络，由Hochreiter和Schmidhuber这两位很难记的作者在1997年提出，主要就是为了解决RNN的梯度消失问题。

在RNN的经典结构中，每一步都会有一个隐层状态，即hidden state，我们记第t步的hidden state为ht ，在LSTM中，作者增加了一个cell state，记为ct。

1. 二者是长度相同的**向量**，长度是一个超参数，即神经网络中的number of units；
2. cell state中可以存储长距离的信息（long-term information）；
3. LSTM的内部机制，可以对cell state中的信息进行擦除、写入和读取（erase,write,read）.

LSTM的三个门分别是：遗忘门（forget gate）、输出门（input gate）和输出门（output gate）。

* **遗忘门**：这个门，根据上一个单元的隐层状态和当前一步的输入，来“设置遗忘门的开合情况”，然后对cell state中的信息进行**选择性遗忘**；
* **输入门**：这个门，也是根据上个单元的隐层状态和当前的输入，来“设置输入门的开合情况”，然后决定往cell state中输入哪些信息，即**选择性输入**；
* **输出门**：这个门，还是根据上个单元的隐层状态和当前输入，来“设置输出门的开合情况”，然后决定从cell state中输出哪些信息，即**选择性输出**，输出的就是当前这一步的隐层状态。

RNN之所以存在梯度消失、无法处理长距离依赖的问题，是因为远处的信息经过多步的RNN单元的计算后不断损失掉了。所以RNN一般只能考虑到较近的步骤对当前输出的影响。LSTM相对于RNN，最大的特点就是“增设了一条管道”——cell state 这条管道，使得每个单元的信息都能够从这里直通其他遥远的单元，而不是一定要通过一个个单元的hidden states来传递信息。这相当于卷积神经网络中常用的“skip connection”技巧。即 `f(x) = f(x) + x`

那么信息是如何通过cell state这个管道呢？我们知道，只要信息在cell state中传递时，遗忘门常开（让信息顺利通过），输入门常闭（不让新的信息进入），那么这条信息就可以想跑多远跑多远；门的这些操作，使得信息进行长距离传递成为了可能。当然上面说的情况比较极端。实际上，这些门一般都不会完全打开或关闭，而是处于各种中间状态。另外，是否让信息传递这么远，也是由当前的上下文决定的。所以并不是说所有的信息都会传递老远，而是当信息真的需要进行长距离传输的时候，LSTM这种机制使得这个过程更加容易。

实际上，LSTM不光是解决了长距离依赖的问题，它的各种门，使得模型的学习潜力大大提升，各种门的开闭的组合，让模型可以学习出自然语言中各种复杂的关系。比如遗忘门的使用，可以让模型学习出什么时候该把历史的信息给忘掉，这样就可以让模型在特点的时候排除干扰。
