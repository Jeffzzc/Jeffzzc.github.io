---
layout:     post
title:      PCN NSDI 2020
subtitle:   Re-architecting Congestion Management in Lossless Ethernet
date:       2020-3-26
author:     Yiran
header-img: img/post-bg-valley3.jpg
catalog: true
tags:
    - Congestion control
    - Transport in Datacenter
---




这篇文章发表在NSDI 2020会议上。笔者花了相当长的时间细读、理解透彻这篇论文（前前后后几个月哈哈）。这篇论文重构了目前无损以太网的拥塞控制，指出了当前拥塞管理架构中两个核心模块（**拥塞识别和速率控制**）存在的根本问题。提出了PCN的拥塞控制协议。

### Experimental observations

论文作者通过构造一个经典场景，给出实验性的观察，指出了现有拥塞控制协议存在的根本问题。实验性的观察比单纯的说理更加具有说服力。

**拓扑**：拓扑的构造看似简单，本质上是CLOS网络单元的高度抽象，很多早期的拥塞控制论文构造的也是类似拓扑。图里的H0、H1、R0、R1都可以替换成一个pod的服务器。
<img width="450" height="250" src="/img/post-pcn-4.png"/>

**流量**：实验的流量是长短流混合，也是作者精巧设计的。F0和F1是长流，H2-H15持续地发送短流，短流的特点是由于长度小于网络的时延带宽乘积（BDP），当发送方被通知拥塞时，短流已经发送完毕，造成短流不被端到端拥塞控制所控制。这种突发性的流量在无损以太网中，只能靠逐跳的PFC所控制，而长流是由端到端拥塞控制所控制的。作者构造的实验，本质上是为了探究网络中突发流量对端到端拥塞控制的干扰。而目前已有的无损以太网拥塞控制，例如DCQCN和Timely，在只有长流的场景下，工作的效果是相似的（无非就是收敛快慢）。PCN就是从这一长短流混合的场景出发，揭示现有拥塞控制的问题。

<img width="450" height="350" src="/img/post-pcn-3.png"/>

**揭示的问题**：

- ```PFC干扰拥塞检测与识别```

  笔者认为揭示的这个问题是这篇论文最核心的贡献。在图中，F0的吞吐降低造成性能损失，原因就是拥塞从P2扩展到P0，扩展的标志就是PFC开始起作用，形成拥塞树。P0收到了PAUSE帧，造成P0队列累积。此时的队列累积并不是因为P0处发生了拥塞，恰恰相反，P0是victim port。而在现有的拥塞控制协议均将队列长度作为拥塞信号，所以造成拥塞信号不准，端上的拥塞控制会误把F0当做造成拥塞的流进行降速。

- 逐步的端到端速率调节与逐跳的流控不匹配

  拥塞扩展的根本原因就是造成拥塞的流没有及时快速地减到合适的速率。换句话说，如果速率减的够快，就不会有拥塞扩展以及可能的victim流的出现。无损以太网中，一个好的拥塞控制算法应当尽可能少的减少PFC的触发（可以阅读DCQCN中关于阈值设置的内容进行理解，同时注意DCQCN也说，它并不能保证PFC完全不触发）。本实验中，短流的速率只能由PFC控制，拥塞控制协议应当将F1快速减到正确的速率，但是由于现有算法的调节都是逐步的，而PFC是逐跳传播的，所以还是出现了从P1到P0的拥塞扩展。

- 线性的速率调节慢

  针对增速，论文指出现有协议的增速都是线性的，既慢又不灵活。





### Design

- **Single Switch Service Model**

  <img width="350" height="350" src="/img/post-pcn-1.png"/>

  $$
  worst \ case \ end-to-end \ delay  <= n \times \frac{P}{R} + \epsilon
  $$

  $P$ the maximum packet size (in bits), $R$ is the rate of the slowest link in bits per second and $\epsilon$ is the cumulative processing delay introduced by switch hops.

  ```If we can find a bound for servicing delay, we can rate limit hosts so that they can never experience queueing dealy.```


- **Network epochs**

  A network epoch is the maximum time that an idle network will take to service one packet from every sending host, regardless of the source, destination or timing of those packets. 

  ```每个host一个epoch只发一个包, 就能有latency的bound.``` 由于时钟不同步，放宽后一个epoch应当是worst case end-to-end delay的两倍.

  $$
  network \ epoch  = 2n \times \frac{P}{R} + \epsilon
  $$


- **QJUMP level**

  如果按照上式的所有host每个epoch发一个包, 随着host数目增多, 吞吐会下降. 对吞吐敏感的应用不友好.

  $$
  throughput = \frac{P}{network \ epoch} \approx \frac{R}{2n}
  $$


  然而, 不同的应用对latency要求不同, 并且实际情况下, 不可能所有的host同时向一个目的端口发数据. 通过引入一个 factor $f$, 根据不同的应用放宽host的数目, 也就是引入QJUMP level. $f$ is a “throughput factor”: as the value of $f$ grows, so does the amount of bandwidth available.

   ```Use hardware priorities to run different QJump levels together, but isolated from each other.```

  $$
  n^{'} = \frac{n}{f}
  $$

  $f = 1$时, 一个epoch能发1个包保证latency; $f = n$时, 一个epoch能发$n$个包保证throughput. QJUMP level利用网络支持的优先级实现: For each priority, assign a distinct value of $f$ , with higher priorities receiving smaller values. Since a small value of $f$ implies an aggressive rate limit, priorities become useful because they are no longer “free”.


### Experiments
- Testbed
- Simulation

  $P = 9kB$, $n = 144$, {$f_{0}$... $f_{7}$} = {144,100,20,10,5,3,2,1}

  <img width="650" height="550" src="/img/post-pcn-2.png"/>


### Thoughts

1. QJUMP实际上是对不同的优先级, 实施不同的速率limit. ```高优先级的速率控制应当比低优先级的速率控制更加aggressive.``` 

2. 在端进行rate limit实际上把网络中的排队一部分转移到了端的socket buffer中, QJUMP resizes socket buffers to apply early back-pressure. end-to-end latency实际上包括在端的排队latency以及网络中排队latency. 如果不resizes socket buffer可能会性能下降. 应用层 rate control 的重要性？

3. Figure 9(b), 结果中QJUMP比pfabric指标好, 说明了端速率控制的必要性？QJUMP对小流甚至能超过pfabric，但是大流不一定. Figure 9(f), between 30%
and 63% worse than pFabric. 

   文中的解释：In the data-mining workload, 85% of all flows transfer fewer than 100kB, but over 80% of the bytes are transferred in flows of greater than 100MB (less than 15% of the total flows). QJUMP’s short epoch intervals cannot sense the difference between large flows, so it does not apply any rate-limiting (scheduling) to them. This results in sub-optimal behavior. A combined approach where QJUMP regulates interactions between large flows and small flows, while DCTCP regulates the interactions between different large flows might improve this.

4. 优先级是针对应用的, 四层以上的, QJUMP解决的也是应用 interference的问题. DCTCP针对三层, 更加底层, 解决小流排队长


### 参考文献

[Re-architecting Congestion Management in Lossless Ethernet](https://www.usenix.org/system/files/nsdi20spring_cheng_prepub_0.pdf)





