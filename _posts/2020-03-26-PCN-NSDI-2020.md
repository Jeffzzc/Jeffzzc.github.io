---
layout:     post
title:      PCN NSDI 2020
subtitle:   Re-architecting Congestion Management in Lossless Ethernet
date:       2020-3-26
author:     Yiran
header-img: img/post-bg-valley3.jpg
catalog: true
tags:
    - Congestion control
    - Transport in Datacenter
---




这篇文章发表在NSDI 2020会议上。笔者花了相当长的时间细读、理解透彻这篇论文。这篇论文重构了目前无损以太网的拥塞控制，指出了当前拥塞管理架构中两个核心模块（**拥塞识别和速率控制**）存在的根本问题。提出了PCN的拥塞控制协议。

### Experimental observations

论文作者通过构造一个经典场景，给出实验性的观察，指出了现有拥塞控制协议存在的根本问题。场景的构造



<img width="650" height="450" src="/img/post-pcn-3.png"/>


### Design

- **Single Switch Service Model**

  <img width="450" height="450" src="/img/post-pcn-1.png"/>

  $$
  worst \ case \ end-to-end \ delay  <= n \times \frac{P}{R} + \epsilon
  $$

  $P$ the maximum packet size (in bits), $R$ is the rate of the slowest link in bits per second and $\epsilon$ is the cumulative processing delay introduced by switch hops.

  ```If we can find a bound for servicing delay, we can rate limit hosts so that they can never experience queueing dealy.```


- **Network epochs**

  A network epoch is the maximum time that an idle network will take to service one packet from every sending host, regardless of the source, destination or timing of those packets. 

  ```每个host一个epoch只发一个包, 就能有latency的bound.``` 由于时钟不同步，放宽后一个epoch应当是worst case end-to-end delay的两倍.

  $$
  network \ epoch  = 2n \times \frac{P}{R} + \epsilon
  $$


- **QJUMP level**

  如果按照上式的所有host每个epoch发一个包, 随着host数目增多, 吞吐会下降. 对吞吐敏感的应用不友好.

  $$
  throughput = \frac{P}{network \ epoch} \approx \frac{R}{2n}
  $$


  然而, 不同的应用对latency要求不同, 并且实际情况下, 不可能所有的host同时向一个目的端口发数据. 通过引入一个 factor $f$, 根据不同的应用放宽host的数目, 也就是引入QJUMP level. $f$ is a “throughput factor”: as the value of $f$ grows, so does the amount of bandwidth available.

   ```Use hardware priorities to run different QJump levels together, but isolated from each other.```

  $$
  n^{'} = \frac{n}{f}
  $$

  $f = 1$时, 一个epoch能发1个包保证latency; $f = n$时, 一个epoch能发$n$个包保证throughput. QJUMP level利用网络支持的优先级实现: For each priority, assign a distinct value of $f$ , with higher priorities receiving smaller values. Since a small value of $f$ implies an aggressive rate limit, priorities become useful because they are no longer “free”.


### Experiments
- Testbed
- Simulation

  $P = 9kB$, $n = 144$, {$f_{0}$... $f_{7}$} = {144,100,20,10,5,3,2,1}

  <img width="750" height="650" src="/img/post-pcn-2.png"/>


### Thoughts

1. QJUMP实际上是对不同的优先级, 实施不同的速率limit. ```高优先级的速率控制应当比低优先级的速率控制更加aggressive.``` 

2. 在端进行rate limit实际上把网络中的排队一部分转移到了端的socket buffer中, QJUMP resizes socket buffers to apply early back-pressure. end-to-end latency实际上包括在端的排队latency以及网络中排队latency. 如果不resizes socket buffer可能会性能下降. 应用层 rate control 的重要性？

3. Figure 9(b), 结果中QJUMP比pfabric指标好, 说明了端速率控制的必要性？QJUMP对小流甚至能超过pfabric，但是大流不一定. Figure 9(f), between 30%
and 63% worse than pFabric. 

   文中的解释：In the data-mining workload, 85% of all flows transfer fewer than 100kB, but over 80% of the bytes are transferred in flows of greater than 100MB (less than 15% of the total flows). QJUMP’s short epoch intervals cannot sense the difference between large flows, so it does not apply any rate-limiting (scheduling) to them. This results in sub-optimal behavior. A combined approach where QJUMP regulates interactions between large flows and small flows, while DCTCP regulates the interactions between different large flows might improve this.

4. 优先级是针对应用的, 四层以上的, QJUMP解决的也是应用 interference的问题. DCTCP针对三层, 更加底层, 解决小流排队长


### 参考文献

[Re-architecting Congestion Management in Lossless Ethernet](https://www.usenix.org/system/files/nsdi20spring_cheng_prepub_0.pdf)





