---
layout:     post
title:      SIMON NSDI 2019
subtitle:   Network Tomography技术在数据中心测量的应用
date:       2019-3-27
author:     Yiran
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - Measurement in Datacenter
    - Network Tomography
    - Machine Learning
---

## [SIMON: A Simple and Scalable Method for Sensing, Inference and Measurement in Data Center Networks](https://www.usenix.org/system/files/nsdi19-geng.pdf)

### 核心思想

用网卡收集到的信息重构网络中队列信息、链路利用率等（Network Tomography技术）

### Measurement 和 monitoring 的三大挑战

准确性、可扩展性、速度

### 已有工作分类

分为 switch-based 和 edge-based.
- switch-based: 可以收集到很细粒度的信息，但是代价高，需要把多个交换机信息拼接在一起、大量带宽去传递收集到的信息。可扩展性不好，INT需要全网部署。并且无法把网络的bottleneck与应用关联
- edge-based：基于端观察到的event(operator或者应用感兴趣的event),与应用relate,不需要修改交换机。可扩展性好。但是准确性不一定。已有工作聚焦于partial view或者approximate view（比如[007](https://www.usenix.org/system/files/conference/nsdi18/nsdi18-arzani.pdf)只关注丢包）


### SIMON的优势 

拥有edge-based的好处，相比switch-based的方法开销小，probe只占大约0.3%带宽。并且near exact reconstruction of network state variables。方法是借助于**Network Tomography(广域网由于大的RTT和不确定的拓扑结构，使得这个技术无法准确重建，追求的结果是某些network state variables的分布，但是数据中心场景下可以)**

### SIMON的几个假设

- 数据中心CLOS的拓扑结构，拓扑已知
- 所有数据包走的路径可以通过traceroute获得
- 产生的probe可以覆盖到所有线性不相关的路径
- **所有NIC的时钟同步**
- 交换机内的排队都在egress

### Design Overview

- probe：64byte TCP或UDP数据包。在网卡，打上tx时间戳 rx时间戳 以及size
- 把整个拓扑考虑成一个图，N个server互相连接。一个server以频率F Hz向其他**任意**K个server（不重复）发送probe，与[Pingmesh](https://yi-ran.github.io/2019/03/27/Pingmesh-SIGCOMM-2015/)选择server的方式不同。收到probe的server以相同的频率回复probe。      出现了频率！！！！————————> 因此论文接下来使用的信号处理理论（以后建模时可按同样的思路考虑信号处理理论是否可行）。论文证明了当K取10-20时 能够覆盖所有的CLOS架构（Fattree）数据中心路径
- **What to reconstruct**：要重建的是一个 *平均队列长度*（在一个reconstruct interval内的）而不是 *瞬时队列长度*，论文使用功率谱密度画图证明了 平均后其实滤掉了 noise       ————————> 因此论文下一步就是确定合适的reconstruct interval，发现interval与网络最大链路速度成反比
- **How to reconstruct**：[LASSO回归](https://blog.csdn.net/qq_30981697/article/details/71438636)

   D = AW + Z

   D: probe 以及data的one way delay矩阵

   A: probe 以及data的incidence矩阵，其实就是走的对应路径（队列）矩阵

   W: probe 以及data的wait time

   Z: noise

![](/img/post-simon-1.jpg)

### Implementation and evaluation
- 两个加速的方法：分层的reconstruction和使用神经网络加速 （并没有在后面的evaluation作真实验证）
- 系统分三部分：prober（用户空间程序）、sensor（NIC driver）、reconstruction engine（一个Spark Streaming cluster，不同reconstruction interval的间隔的计算是相互独立的 可以每一个server计算一个reconstruction interval）
- 其实不太好验证真实的queue与reconstruction的queue。在一个小拓扑上用了NetFPGA获取真实队列长度。使用交叉验证的方法在另外两个拓扑上验证
- use case  重构中发现突然出现了 5ms的排队延迟 （正常应该是500us），**但是没有丢包**，后来发现是因为设置了优先级队列，被高优先级的流blocked了


### 可能存在的问题？
重构的信息：队列长度、link utilization（以及component）是全部的吗？packet drop?
对targeted的流 不仅需要其本身的data packet做重构 也需要probe packet 来全覆盖网络
