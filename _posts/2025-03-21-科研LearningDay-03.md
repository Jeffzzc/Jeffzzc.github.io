---
layout:     post
title:      科研Day-03
subtitle:   RAG
date:       2025-4-20
author:     Jeffzzc
header-img: img/home-bg-sky2.jpg
catalog: true
tags:
    - 科研
---
### RAG

#### 为什么会有RAG

随着ChatGPT、文心一言、通义千问、LLama系列等大模型的广泛应用，各行业尝试将其引入业务流程。这些模型在知识、理解和推理方面展现了卓越的能力，在复杂交互场景中表现尤为突出。

然而，这些模型仍然存在一些无法忽视的局限性。其中，**领域知识缺乏**是最明显的问题。大模型的知识来源于训练数据，这些数据主要来自公开的互联网和开源数据集，无法覆盖特定领域或高度专业化的内部知识。**信息过时**则指模型难以处理实时信息，因为训练过程耗时且成本高昂，模型一旦训练完成，就难以获取和处理新信息。

此外，**幻觉问题**是另一个显著的局限，模型基于概率生成文本，有时会输出看似合理但实际错误的答案。最后，**数据安全性**在企业应用中尤为重要，如何在确保数据安全的前提下，使大模型有效利用私有数据进行推理和生成，是一个具有挑战性的问题。

#### RAG定义

RAG（Retrieval-Augmented Generation，检索增强生成）通过将非参数化的外部知识库、文档与大模型相结合，RAG使模型在生成内容之前，能够先检索相关信息，从而弥补模型在知识专业性和时效性上的不足，减少生成不确定性，在确保数据安全的同时，充分利用领域知识和私有数据。

RAG技术是一种结合检索与生成的自然语言处理（NLP）模型架构。这个技术由Facebook AI于2022年提出，旨在提升生成式模型在处理开放域问答、对话生成等复杂任务中的性能。RAG通过引入外部知识库，利用检索模块（Retriever）从大量文档中提取相关信息，并将这些信息传递给生成模块（Generator），从而生成更准确且有用的回答。

**RAG模型的核心思想在于通过检索与生成的有机结合，弥补大模型在处理领域问题和实时任务时的不足。** 传统的生成模型在面对复杂问题时，往往由于知识储备不足，生成出错误或无关的回答。而RAG通过检索模块获取相关的背景信息，使生成模块能够参考这些信息，从而生成更具可信度和准确性的答案。这种方法不仅增强了生成内容的准确性，还提高了模型在应对特定领域知识和动态信息时的适应能力。

#### RAG标准技术流程

RAG标准流程由 **索引** （Indexing）、 **检索** （Retriever）和 **生成** （Generation）三个核心阶段组成。

1. 索引阶段，通过处理多种来源多种格式的文档提取其中文本，将其切分为标准长度的文本块（chunk），并进行嵌入向量化（embedding），向量存储在向量数据库（vector database）中。
2. 检索阶段，用户输入的查询（query）被转化为向量表示，通过相似度匹配从向量数据库中检索出最相关的文本块。
3. 最后生成阶段，检索到的相关文本与原始查询共同构成提示词（Prompt），输入大语言模型（LLM），生成精确且具备上下文关联的回答。

通过这一流程，RAG实现了检索与生成的有机结合，显著提升了LLM在领域任务中的准确性和实时性。

**索引是RAG系统的基础环节** ，包含四个关键步骤。

1. 首先，将各类数据源及其格式（如书籍、教材、领域数据、企业文档等，txt、markdown、doc、ppt、excel、pdf、html、json等格式）统一解析为纯文本格式。
2. 接着，根据文本的语义或文档结构，将文档分割为小而语义完整的文本块（chunks），确保系统能够高效检索和利用这些块中包含的信息。
3. 然后，使用文本嵌入模型（embedding model），将这些文本块向量化，生成高维稠密向量，转换为计算机可理解的语义表示。
4. 最后，将这些向量存储在向量数据库(vector database)中，并构建索引，完成知识库的构建。这一流程成功将外部文档转化为可检索的向量，支撑后续的检索和生成环节。

**检索是连接用户查询与知识库的核心环节。** 首先，用户输入的问题通过同样的文本嵌入模型转换为向量表示，将查询映射到与知识库内容相同的向量空间中。通过相似度度量方法，检索模块从向量数据库中筛选出与查询最相关的前K个文本块，这些文本块将作为生成阶段输入的一部分。通过相似性搜索，检索模块有效获取了与用户查询切实相关的外部知识，为生成阶段提供了精确且有意义的上下文支持。

**生成是RAG流程中的最终环节** ，将检索到的相关文本块与用户的原始查询整合为增强提示词（Prompt），并输入到大语言模型（LLM）中。LLM基于这些输入生成最终的回答，确保生成内容既符合用户的查询意图，又充分利用了检索到的上下文信息，使得回答更加准确和相关，充分使用到知识库中的知识。通过这一过程，RAG实现了具备领域知识和私有信息的精确内容生成。

#### 从0到1快速搭建RAG应用

主体流程：

* **LangChain：** 提供用于构建LLM RAG的应用程序框架。
* **索引流程：** 使用 **pypdf** 对文档进行解析并提取信息；随后，采用 **RecursiveCharacterTextSplitter** 对文档内容进行分块（chunks）；最后，利用 **bge-small-zh-v1.5** 将分块内容进行向量化处理，并将生成的向量存储在 **Faiss** 向量库中。
* **检索流程：** 使用 **bge-small-zh-v1.5** 对用户的查询（Query）进行向量化处理；然后，通过 **Faiss** 向量库对查询向量和文本块向量进行相似度匹配，从而检索出与用户查询最相似的前 **top-k** 个文本块（chunk）。
* **生成流程：** 通过设定提示模板（Prompt），将用户的查询与检索到的参考文本块组合输入到 **Qwen** 大模型中，生成最终的 RAG 回答。
