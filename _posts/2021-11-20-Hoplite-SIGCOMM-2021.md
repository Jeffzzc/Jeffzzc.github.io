---
layout:     post
title:      Hoplite SIGCOMM 2021
subtitle:   Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
date:       2021-11-20
author:     Yiran
header-img: img/post-bg-bear.jpg
catalog: true
tags:
    - Distributed Sytems
---

ç¬¬ä¸€ä¸ªä¸ºåŸºäºä»»åŠ¡çš„åˆ†å¸ƒå¼ç³»ç»Ÿæä¾›æœ‰æ•ˆçš„ Collective Communication æ”¯æŒçš„å·¥ä½œã€‚

### Background

- Task-based distributed systems
  
  asynchronous and dynamic computation: a caller can dynamically invoke a task ğ´, which immediately returns an object future, i.e. a reference to the eventual return value. By passing the future as an argument, the caller can specify another task ğµ that uses the return value of ğ´ even before ğ´ finishes. The task-based system is responsible for scheduling workers to execute tasks ğ´ and ğµ and transferring the result of ğ´ to ğµ between the corresponding workers.

  <img width="450" height="800" src="/img/post-hoplite-0.png"/>


- Collective Communications

  e.g., **broadcast, reduce**

  ç°æœ‰çš„collective communication library: OpenMPI, MPICH, Horovod, Gloo, and NCCL. 


### Motivation

- ç°æœ‰çš„collective communication libraryçš„é—®é¢˜
  
  1. The communication pattern has to be **statically** defined before runtime.  åªå¯¹å…·æœ‰bulk-synchronous parallel æ¨¡å‹çš„åº”ç”¨å‹å¥½ã€‚ä¾‹å¦‚ï¼šall the workers compute on their partitioned set of training data and synchronize the model parameters using allreduce.

  2. å®¹é”™æ€§ä¸ä½³. When any worker fails, all the workers participating in the collective communication hang, and applications are responsible for fault tolerance: checkpointing the entire application periodically (e.g., per-hour), and when a process fails, the entire application rolls back to a checkpoint and re-execute.


**Goal: Bring the efficiency of collective communication to dynamic and asynchronous task-based applications**.

 - ```A collective communication layer for a task-based system should adjust data transfer schedule at runtime based on task and object arrivals.```

 - ```A collective communication layer for task-based systems has to be fault-tolerant: allowing well-behaving tasks to make progress when a task fails and allowing the failed task to rejoin the collective communication after recovery.```




### Design

æ ¸å¿ƒè®¾è®¡ä¸¤ç‚¹ï¼š

- decentralized fault-tolerant coordination of data transfer for reduce and broadcast 
- pipelining of object transfers both across nodes and between tasks and the object store.

**Hoplite Workflow**

**Object Directory Service**

ä¸ªäººè®¤ä¸ºè¿™ä¸ª object store çš„å¼•å…¥æ˜¯å®ç° runtime schedule çš„å…³é”®

**Pipelining**

å¯¹ object store è¯»å–æ•°æ®çš„ä¼˜åŒ–ï¼šHoplite uses pipelining to achieve low-latency transfer between processes and across nodes for large objects. This is implemented by enabling a receiver node to fetch an object that is incomplete in a source node.

**Receiver-Driven Collective Communication**

```Broadcastçš„ä¼˜åŒ–ï¼š```If all receivers simply fetch the object from the sender, the performance will be restricted by the senderâ€™s upstream bandwidth. Traditional collective communication libraries can generate a **static tree** where the root is the sender node to mitigate the throughput bottleneck. Hopliteâ€™s receiver-driven coordination scheme is to achieve a similar effect but using decentralized protocols: use receivers who receives the object earlier than the rest as intermediates to construct a broadcast tree.

```Reduceæ›´å¤æ‚ï¼š ```Broadcast is simpler because a receiver can fetch the object from any sender, and Hoplite thus has more flexibility to adapting data transfer schedule. For reduce, we need to make sure all the objects are reduced once and only once: when one object is added into a partial reduce result, the object should not be added into any other partial results.

Assume $n$ objects. Without the support of collective communication in task-based distributed systems, each node sends the object to a
single receiver. network latency is $L$ , network bandwidth is $B$, and the object size is $S$. Total reduce time : $L+ \frac{nS}{B}$ . ä¸ºäº†ç¼“è§£receiverä½œä¸ºå”¯ä¸€rootçš„å¸¦å®½ç“¶é¢ˆï¼Œè€ƒè™‘$d-nary$ tree, the total running time is $L+log_{d}n+ \frac{nS}{B}$. It reduces the latency due to the bandwidth constraint but incurs additional latency because the height of the tree grows to $log_{d}n$. The **optimal choice of $d$** depends on the network characteristics, the size of the object, and the number of participants.

  <img width="950" height="650" src="/img/post-hoplite-2.png"/>

**fault-Tolerant Collective Communication**

ä¹Ÿæ˜¯åˆ†çš„broadcastå’Œreduceè®¨è®º

### Implementation

Python front end

object directory service: gRPC

### Evaluation

<img width="1100" height="850" src="/img/post-hoplite-3.png"/>


### Thinking

éå¸¸å…¸å‹çš„ç³»ç»Ÿè®¾è®¡æ–‡ç« ã€‚æ³¨æ„æ€è€ƒä¸ºä»€ä¹ˆå¼•å…¥å„ä¸ªdesignã€‚


### å‚è€ƒæ–‡çŒ®

[Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems](https://dl.acm.org/doi/10.1145/3452296.3472897)
