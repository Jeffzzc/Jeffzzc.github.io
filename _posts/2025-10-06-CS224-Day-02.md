---
layout:     post
title:      CS224Day-02
subtitle:   Dependency Parsing HW && RNNs
date:       2025-10-06
author:     Jeffzzc
header-img: img/home-bg-sky2.jpg
catalog: true
tags:
    - CS224
---
### Lecture 4-6 Dependency Parsing HW && RNNs

#### **Dependency Parsing HW**

æŠŠæœ€åçš„Assignment2å†™å®Œäº†ï¼Œä¸‹é¢æ˜¯æœ€åçš„run.pyè·‘åˆ†ç»“æœä»¥åŠä»£ç ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CS224N 2023-2024: Homework 2
run.py: Run the dependency parser.
Sahil Chopra <schopra8@stanford.edu>
Haoshen Hong <haoshen@stanford.edu>
"""
from datetime import datetime
import os
import pickle
import math
import time
import argparse

from torch import nn, optim
import torch
from tqdm import tqdm

from parser_model import ParserModel
from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter

parser = argparse.ArgumentParser(description='Train neural dependency parser in pytorch')
parser.add_argument('-d', '--debug', action='store_true', help='whether to enter debug mode')
args = parser.parse_args()

# -----------------
# Primary Functions
# -----------------
def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):
    """ Train the neural dependency parser.

    @param parser (Parser): Neural Dependency Parser
    @param train_data ():
    @param dev_data ():
    @param output_path (str): Path to which model weights and results are written.
    @param batch_size (int): Number of examples in a single batch
    @param n_epochs (int): Number of training epochs
    @param lr (float): Learning rate
    """
    best_dev_UAS = 0


    ### YOUR CODE HERE (~2-7 lines)
    ### TODO:
    ###      1) Construct Adam Optimizer in variable `optimizer`
    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`
    ###         reduction (default)
    ###
    ### Hint: Use `parser.model.parameters()` to pass optimizer
    ###       necessary parameters to tune.
    ### Please see the following docs for support:
    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html
    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss

    optimizer = optim.Adam(parser.model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss(reduction='mean')

    ### END YOUR CODE

    for epoch in range(n_epochs):
        print("Epoch {:} out of {:}".format(epoch + 1, n_epochs))
        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)
        if dev_UAS > best_dev_UAS:
            best_dev_UAS = dev_UAS
            print("New best dev UAS! Saving model.")
            torch.save(parser.model.state_dict(), output_path)
        print("")


def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):
    """ Train the neural dependency parser for single epoch.

    Note: In PyTorch we can signify train versus test and automatically have
    the Dropout Layer applied and removed, accordingly, by specifying
    whether we are training, `model.train()`, or evaluating, `model.eval()`

    @param parser (Parser): Neural Dependency Parser
    @param train_data ():
    @param dev_data ():
    @param optimizer (nn.Optimizer): Adam Optimizer
    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function
    @param batch_size (int): batch size

    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data
    """
    parser.model.train() # Places model in "train" mode, i.e. apply dropout layer
    n_minibatches = math.ceil(len(train_data) / batch_size)
    loss_meter = AverageMeter()

    with tqdm(total=(n_minibatches)) as prog:
        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):
            optimizer.zero_grad()   # remove any baggage in the optimizer
            loss = 0. # store loss for this batch here
            train_x = torch.from_numpy(train_x).long()
            train_y = torch.from_numpy(train_y.nonzero()[1]).long()

            ### YOUR CODE HERE (~4-10 lines)
            ### TODO:
            ###      1) Run train_x forward through model to produce `logits`
            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.
            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss
            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)
            ###         are the predictions (y^ from the PDF).
            ###      3) Backprop losses
            ###      4) Take step with the optimizer
            ### Please see the following docs for support:
            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step

            logits = parser.model(train_x)
            loss = loss_func(logits, train_y)
            loss.backward()
            optimizer.step()

            ### END YOUR CODE
            prog.update(1)
            loss_meter.update(loss.item())

    print ("Average Train Loss: {}".format(loss_meter.avg))

    print("Evaluating on dev set",)
    parser.model.eval() # Places model in "eval" mode, i.e. don't apply dropout layer
    dev_UAS, _ = parser.parse(dev_data)
    print("- dev UAS: {:.2f}".format(dev_UAS * 100.0))
    return dev_UAS


if __name__ == "__main__":
    debug = args.debug

    assert (torch.__version__.split(".") >= ["1", "0", "0"]), "Please install torch version >= 1.0.0"

    print(80 * "=")
    print("INITIALIZING")
    print(80 * "=")
    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)

    start = time.time()
    model = ParserModel(embeddings)
    parser.model = model
    print("took {:.2f} seconds\n".format(time.time() - start))

    print(80 * "=")
    print("TRAINING")
    print(80 * "=")
    output_dir = "results/{:%Y%m%d_%H%M%S}/".format(datetime.now())
    output_path = output_dir + "model.weights"

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)

    if not debug:
        print(80 * "=")
        print("TESTING")
        print(80 * "=")
        print("Restoring the best model weights found on the dev set")
        parser.model.load_state_dict(torch.load(output_path))
        print("Final evaluation on test set",)
        parser.model.eval()
        UAS, dependencies = parser.parse(test_data)
        print("- test UAS: {:.2f}".format(UAS * 100.0))
        print("Done!")

```

è·‘åˆ†ç»“æœï¼š

```python
================================================================================
TESTING
================================================================================
Restoring the best model weights found on the dev set
/workspace/CS224n/a2/run.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  parser.model.load_state_dict(torch.load(output_path))
Final evaluation on test set
2919736it [00:00, 57512000.83it/s]                                                                                                                                                                                                                                                             
- test UAS: 89.00
Done!
```

#### Language Models

Language models compute the probability of occurrence of a number of words in a particular sequence. The probability of a sequence of m words w1,...,wm is denoted as P(w1,...,wm).

##### n-gram Language Models

To compute the probabilities mentioned above, the count of each n-gram could be compared against the frequency of each word. This is called an n-gram Language Model. For instance, if the model takes bi-grams, the frequency of each bi-gram, calculated via combining a word with its previous word, would be divided by the frequency of the corresponding uni-gram.

ä½†æ˜¯ä»ä¸Šé¢çš„åˆ†æè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¹Ÿä¸éš¾å‘ç°è¿™æ ·å¾—åˆ°çš„LMçš„ä¸€äº›ä¸¥é‡é—®é¢˜ï¼š

1. **ç¨€ç–æ€§é—®é¢˜**æˆ‘ä»¬å‰é¢æåˆ°è¿‡åˆ†å­åˆ†æ¯å¾ˆå®¹æ˜“ä¸º0ï¼Œå°±æ˜¯ç”±äºN-gramçš„ç¨€ç–æ€§é€ æˆçš„ï¼ŒNè¶Šå¤§ï¼Œè¿™ç§ç¨€ç–æ€§çš„é—®é¢˜å°±è¶Šä¸¥é‡ï¼Œå¾ˆå¯èƒ½ä½ ç»Ÿè®¡çš„å¤§å¤šæ•°N-graméƒ½ä¸å­˜åœ¨ã€‚
2. **å­˜å‚¨é—®é¢˜**è™½ç„¶è¿™ç§åŸºäºè®¡æ•°çš„LMçš„å¾ˆç®€å•ï¼Œä½†æ˜¯æˆ‘ä»¬å¿…é¡»ç©·ä¸¾å‡ºé¢„æ–™ä¸­æ‰€æœ‰å¯èƒ½çš„N-gramï¼Œå¹¶é€ä¸€å»è®¡æ•°ã€ä¿å­˜ï¼ŒNä¸€æ—¦å¤§èµ·æ¥çš„è¯ï¼Œæ¨¡å‹çš„å¤§å°å°±ä¼šé™¡å¢ã€‚
3. Nå¤ªå¤§äº†ä¼šæœ‰è¿‡äºç¨€ç–ã€éš¾ä»¥ä¿å­˜çš„é—®é¢˜ï¼Œç›¸åNå¤ªå°äº†ï¼Œæ¨¡å‹çš„é¢„æµ‹**å‡†ç¡®ç‡**å°±ä¼šæ˜æ˜¾é™ä½ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œè¯­è¨€æ¨¡å‹æœ€ç›´æ¥çš„ç”¨é€”ï¼Œå°±æ˜¯æ–‡æœ¬è”æƒ³ï¼Œå’Œæ–‡æœ¬ç”Ÿæˆäº†ã€‚ å¯¹äºæ–‡æœ¬è”æƒ³ï¼Œè¿™ç§åŸºäºè®¡æ•°çš„LMä¹Ÿä¸æ˜¯ä¸èƒ½ç”¨ï¼Œæ¯•ç«Ÿè”æƒ³å‡ºæ¥çš„è¯ç¡®å®åœ¨ç»Ÿè®¡æ„ä¹‰ä¸Šæ˜¯æ›´åŠ é¢‘ç¹çš„ï¼Œæ‰€ä»¥ç”¨æˆ·ç›´æ¥æ„Ÿå—ä¸å‡ºå®ƒçš„ç¼ºç‚¹ã€‚

ä½†å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¥è¯´ï¼Œè¿™ç§åŸºäºè®¡æ•°çš„LMåˆ™æœ‰å¾ˆå¤§é—®é¢˜äº†ã€‚ä¼šå‡ºç°å®ƒè¯´ç€è¯´ç€å°±è¯´åäº†ï¼Œå†è¯´ä¸€ä¼šåˆåäº†ï¼Œå¯¼è‡´æ•´ä¸ªæ–‡æœ¬æ²¡æœ‰ä¸€ä¸ªæ¸…æ™°çš„ä¸»é¢˜ã€‚å¾ˆæ˜æ˜¾ï¼Œè¿™é‡Œè®­ç»ƒçš„LMé‡‡ç”¨çš„N-gramä¸€å®šä¸å¤§ã€‚ä½†æ˜¯Nä¸€å¤§èµ·æ¥ï¼Œè®­ç»ƒèµ·æ¥åˆå¾ˆå›°éš¾ã€‚è¿™å°±æ˜¯åŸºäºè®¡æ•°çš„LMçš„å›°å¢ƒã€‚

##### window-based neural model

This model learns a distributed representation of words, along with the probability function for word sequences expressed in terms of these representations. The input word vectors are used by both the hidden layer and the output layer.Equation 4 represents Figure 1 and shows the parameters of the softmax() function, consisting of the standard tanh() function (i.e. the hidden layer) as well as the linear function, W(3)x + b(3), that captures all the previous n input word vectors.
 `y =softmax(W(2)tanh(W(1)x +b(1)) +W(3)x +b(3))`

Improvements over n-gram LM:
 â€¢ No sparsity problem
 â€¢ Donâ€™t need to store all observed n-grams

Remaining problems:
 â€¢ Fixed window is too small
 â€¢ Enlarging window enlarges ğ‘Š
 â€¢ Window can never be large enough!
 â€¢ ğ‘¥(1) and ğ‘¥(2) are multiplied by completely different weights in ğ‘Š.
 No symmetry in how the inputs are processed.

##### Recurrent Neural Networks (RNN)

RNNå³å¾ªç¯ç¥ç»ç½‘ç»œï¼Œä¸ºä½•å«å¾ªç¯å‘¢ï¼Ÿå› ä¸ºä¸ç®¡RNNæœ‰å¤šé•¿ï¼Œå®ƒå®é™…ä¸Šéƒ½æ˜¯åœ¨**åŒä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ä¸æ–­å¾ªç¯** ï¼Œå› æ­¤ä»–ä»¬çš„**æƒé‡éƒ½æ˜¯ä¸€æ ·**çš„ï¼Œåªæ˜¯æ ¹æ®è¾“å…¥çš„ä¸åŒï¼Œè€Œäº§ç”Ÿä¸åŒçš„è¾“å‡ºã€‚

æœ‰äº†è¿™æ ·çš„ç»“æ„ï¼Œä½¿å¾—RNNå…·æœ‰ä»¥ä¸‹è¿™äº›ä¼˜ç‚¹ï¼š

1. å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥
2. æ¨¡å‹çš„å¤§å°ä¸éšè¾“å…¥é•¿åº¦çš„å¢åŠ è€Œå¢å¤§
3. åé¢æ­¥éª¤çš„å¤„ç†è¿‡ç¨‹å¯ä»¥åˆ©ç”¨åˆ°å‰é¢çš„ä¿¡æ¯
4. æ¯ä¸€æ­¥å…·æœ‰ç›¸åŒçš„æƒé‡çŸ©é˜µï¼Œä½¿å¾—ç½‘ç»œå¯ä»¥åˆ©ç”¨ä¸åŒè¾“å…¥çš„ç›¸ä¼¼æ€§

ç„¶è€Œï¼ŒRNNä¹Ÿæœ‰å…¶ç¼ºç‚¹ï¼š

1. è®¡ç®—æ…¢ã€‚è¿™æ˜¯ç”±äºå®ƒå¿…é¡»åœ¨å¤„ç†å®Œä¸Šä¸€æ­¥åæ‰èƒ½è¿›è¡Œä¸‹ä¸€æ­¥çš„è®¡ç®—ã€‚
2. å½“è¾“å…¥è¿‡é•¿çš„æ—¶å€™ï¼Œéš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚

###### Trainning RNN

 â€¢ Get a big corpus of text which is a sequence of words
 â€¢ Feed into RNN-LM; compute output distribution for every step t.
 â€¢ Loss function on step t is cross-entropy between predicted probability distribution, and the true next word
 â€¢ Average this to get overall loss for entire training set

##### RNNçš„é—®é¢˜

æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ˆvanishing/exploding gradientsï¼‰

æˆ‘ä»¬ç§°è¿™ä¸ªç»å…¸çš„RNNç»“æ„ï¼Œä¸ºvanilla RNNæˆ–è€…simple RNNï¼Œè¿™ä¸ªvanillaçš„æ„æ€æ˜¯â€œæ™®é€šçš„ï¼Œæ¯«æ— ç‰¹è‰²çš„â€ï¼Œåœ¨è®ºæ–‡ä¸­æˆ‘ä»¬ä¼šç»å¸¸çœ‹åˆ°ã€‚

è¿™é‡Œè®¡ç®—Jå…³äºh_tçš„æ¢¯åº¦çš„æ—¶å€™å°±ä¼šå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦ä¸‹é™çš„é—®é¢˜ï¼Œ(**å½“Wå¾ˆå°æˆ–è€…å¾ˆå¤§ï¼ŒåŒæ—¶iå’Œjç›¸å·®å¾ˆè¿œçš„æ—¶å€™)**

* **ã€Œæ¢¯åº¦æ¶ˆå¤±ã€** æ—¶ï¼Œä¼šè®©RNNåœ¨æ›´æ–°çš„æ—¶å€™ï¼Œåªæ›´æ–°é‚»è¿‘çš„å‡ æ­¥ï¼Œè¿œå¤„çš„æ­¥å­å°±æ›´æ–°ä¸äº†ã€‚æ‰€ä»¥é‡åˆ°â€œé•¿è·ç¦»ä¾èµ–â€çš„æ—¶å€™ï¼Œè¿™ç§RNNå°±æ— æ³•handleäº†ã€‚
* **ã€Œæ¢¯åº¦çˆ†ç‚¸ã€** æ—¶ï¼Œä¼šå¯¼è‡´åœ¨æ¢¯åº¦ä¸‹é™çš„æ—¶å€™ï¼Œæ¯ä¸€æ¬¡æ›´æ–°çš„æ­¥å¹…éƒ½è¿‡å¤§ï¼Œè¿™å°±ä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹å˜å¾—ååˆ†å›°éš¾ã€‚

###### **å¦‚ä½•è§£å†³vanilla RNNä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€çˆ†ç‚¸é—®é¢˜**

- **æ¢¯åº¦çˆ†ç‚¸é—®é¢˜çš„è§£å†³**

å‰é¢è®²åˆ°æ¢¯åº¦çˆ†ç‚¸å¸¦æ¥çš„ä¸»è¦é—®é¢˜æ˜¯åœ¨æ¢¯åº¦æ›´æ–°çš„æ—¶å€™æ­¥å¹…è¿‡å¤§ã€‚é‚£ä¹ˆæœ€ç›´æ¥çš„æƒ³æ³•å°±æ˜¯é™åˆ¶è¿™ä¸ªæ­¥é•¿ï¼Œæˆ–è€…æƒ³åŠæ³•è®©æ­¥é•¿å˜çŸ­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€œæ¢¯åº¦ä¿®å‰ªï¼ˆgradient clippingï¼‰â€çš„æŠ€å·§æ¥åº”å¯¹æ¢¯åº¦çˆ†ç‚¸ã€‚å³å½“æ­¥é•¿è¶…è¿‡æŸé˜ˆå€¼ï¼Œé‚£å°±æŠŠæ­¥é•¿ç¼©å‡åˆ°è¿™ä¸ªé˜ˆå€¼ã€‚

- **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„è§£å†³**

é‚£ä¹ˆå¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜å‘¢ï¼Ÿæ¢¯åº¦æ¶ˆå¤±å¸¦æ¥çš„æœ€ä¸¥é‡é—®é¢˜åœ¨äºï¼Œåœ¨æ›´æ–°å‚æ•°æ—¶ï¼Œç›¸æ¯”äºä¸´è¿‘çš„stepï¼Œé‚£äº›è¾ƒè¿œçš„stepå‡ ä¹æ²¡æœ‰è¢«æ›´æ–°ã€‚ä»å¦ä¸€ä¸ªè§’åº¦è®²ï¼Œæ¯ä¸€ä¸ªstepçš„ä¿¡æ¯ï¼Œç”±äºæ¯ä¸€æ­¥éƒ½åœ¨è¢«åå¤ä¿®æ”¹ï¼Œå¯¼è‡´è¾ƒè¿œçš„stepçš„ä¿¡æ¯éš¾ä»¥ä¼ é€’è¿‡æ¥ï¼Œå› æ­¤ä¹Ÿéš¾ä»¥è¢«æ›´æ–°ã€‚å¯ä»¥çœ‹å‡ºï¼Œhidden stateåœ¨ä¸æ–­è¢«é‡å†™ï¼Œè¿™æ ·çš„è¯ï¼Œç»è¿‡å‡ æ­¥çš„ä¼ é€’ï¼Œæœ€å¼€å§‹çš„ä¿¡æ¯å°±å·²ç»æ‰€å‰©æ— å‡ äº†ã€‚è¿™æ ¹å‰é¢åœ¨è®¨è®ºæ¢¯åº¦æ¶ˆå¤±çš„é‚£ä¸ªåŒ…å«æŒ‡æ•°è®¡ç®—çš„å…¬å¼æ˜¯é¥ç›¸å‘¼åº”çš„ï¼Œéƒ½åæ˜ äº†vanilla RNNæ— æ³•å¯¹ä»˜é•¿è·ç¦»ä¾èµ–çš„é—®é¢˜ã€‚

- äºæ˜¯å°±å¼•å‡ºäº†æ˜å¤©çš„ä¸»é¢˜LSTM
